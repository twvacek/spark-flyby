UPDATE December 15, 2021: I wrote this code for a 2015 Spark Summit talk. After a personal communication, I recovered this code base and reposted it here, also adding the Apache license. Dependencies need to be updated. One should consider switching from Breeze to Tensorflow Java (EagerSession) as a linear algebra library.

DistributedDenseMatrix is a toy class for evaluating cartesian dependency problems on Spark.
One specifies a matrix dimension and block dimension.  Blocks are all of uniform size
except possibly at the last index in each dimension.  The basic DenseMatrix object
contains global blocking information, and a key-value RDD [((row_block, col_block), breeze.linalg.DenseMatrix)].
In any operation where a partitioner is accepted, a column major partitioner is used, which places
the blocks in column major order in the spark partitions.  Having fewer but larger blocks partitions allows work to be
shifted from Spark to Breeze, and the extreme case of 1 block per partition is what I used in the evaluation.

This implements several approaches to the problem.
cartesian: Spark's built-in cartesian for these kinds of problems
shuffle: Typical representative of shuffle-based matrix multiplication.  Somewhat clever partitioner to ensure
 only a single shuffle.
flybyInner: Uses flyby primitive, but uses inner-product multiplications to fill in submatrices of result
flybyOuter: Users flyby primitive, but aggregates result from components of full dimension

The inner and outer flavors of flyby differ primarily in the expected partitioning of their args.




val matB = DM.newRandomMatrix(25000, 25000, 10, 10, 100, sc)
matB.persist().count
matB.shuffleMultiply(matB).frobNormSq
//took 1308 seconds


val matB = DM.newRandomMatrix(25000, 25000, 1, 100, 100, sc)
matB.persist().count
val st = System.nanoTime; prod = matB.flyByOuter(matB); sp = System.nanoTime

scala> st/1e9
res14: Double = 2013418.536998622

scala> sp/1e9
res15: Double = 2014058.177673988

//took 640 seconds

 spark-submit --master yarn-client --num-executors 5 --executor-cores 30 --driver-memory 10g \
 --executor-memory 50g --class com.trgr.rd.sparkflyby.TimingTest --conf spark.broadcast.compress=false \
 ./SparkFlyby-1.0-SNAPSHOT.jar test_output_2 10000 3 6

 spark-submit --master yarn-client --num-executors 5 --executor-cores 30 --driver-memory 30g \
  --executor-memory 50g --class com.trgr.rd.sparkflyby.TimingTest --conf spark.broadcast.compress=false \
  ./SparkFlyby-1.0-SNAPSHOT.jar test_output_realz 30000  12
